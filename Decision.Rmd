---
title: "Decision Trees"
parent: "Machine Learning"
has_children: false
nav_order: 1
mathjax: true ## Switch to false if this page has no equations or other math rendering.
---

# Decision Trees

Decision trees are among the most common and useful machine learning methodologies. While they are a relatively simple method, they are incredibly easy to understand and implement for both classification and regression problems.

A decision tree "grows" by creating a cutoff point (often called a split) at a single point in the data that maximizes accuracy. The tree's prediction is then based on the mean of the region that results from the input data.

For both regression and classification trees, it is important to optimize the number of splits that we allow the tree to make. If there is no limit, the trees would be able to create as many splits as the data will allow. This would mean the tree could perfectly "predict" every value from the training dataset, but would perform terribly out of sample. As such, it is important to keep a reasonable limit on the number of splits.

For regression trees, the decision to split along a continuum of values is often made by minimizing the residual sum of squares:

$$
minimize \sum(y-prediciton)^2
$$
This should be highly reminicent of ordinary least squares. Where this differs is in the number of splits created, the binary nature of the splits, and its nonlinear nature.

The methodology behind classificiation is very similar, except the splits are decided by minmimzing purity, such as the Gini index:

$$
G= 1 - \sum_{i = 1}^{C} (p_{i})^2 
$$

The goal here is to create regions with as of classifications as possible, as such, a smaller Gini index implies a more pure region.

## Keep in Mind



## Also Consider



# Implementations

```
titanic <- read.csv("https://www.kaggle.com/c/titanic/data")

```



Remember that you can use inline math, e.g. $$x + y$$. In general, you should render variables in math mode ($$X$$, $$Y$$, etc.)

You can also render math in display mode:

$$
\int_a^b f(x)dx
$$

## Keep in Mind

- LIST OF IMPORTANT THINGS TO REMEMBER ABOUT USING THE TECHNIQUE

## Also Consider

- LIST OF OTHER TECHNIQUES THAT WILL COMMONLY BE USED ALONGSIDE THIS PAGE'S TECHNIQUE
- (E.G. LINEAR REGRESSION LINKS TO ROBUST STANDARD ERRORS),
- OR INSTEAD OF THIS TECHNIQUE
- (E.G. PIE CHART LINKS TO A BAR PLOT AS AN ALTERNATIVE)
- WITH EXPLANATION
- INCLUDE LINKS TO OTHER LOST PAGES WITH THE FORMAT [Description](https://lost-stats.github.io/Category/page_name.html). Categories include Data_Manipulation, Geo-Spatial, Machine_Learning, Model_Estimation, Presentation, Summary_Statistics, Time_Series, and Other

# Implementations

## NAME OF LANGUAGE/SOFTWARE 1

```identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs
Commented code demonstrating the technique
```

## NAME OF LANGUAGE/SOFTWARE 2 WHICH HAS MULTIPLE APPROACHES

There are two ways to perform this technique in language/software 2.

First, explanation of what is different about the first way:

```identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs
Commented code demonstrating the technique
```

Second, explanation of what is different about the second way:

```identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs
Commented code demonstrating the technique
```
